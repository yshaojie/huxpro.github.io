---
layout:     post
title:      "k8s生产集群部署"
date:       2020-05-13 17:03:00
author:     "聼雨夜"
catalog: true
tags:
    - docker
    - k8s
---
#### 环境初始化

##### 软件版本
```bash
k8s_version=1.18.2
docker_version=19.03.8
calico_version=3.13
```
##### 添加网桥过滤
```bash
# 添加网桥过滤及地址转发
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0

EOF

# 加载 br_netfilter 模块
modprobe br_netfilter
# 查看是否加载
lsmod | grep br_netfilter

# 加载网桥过滤配置文件
sysctl -p /etc/sysctl.d/k8s.conf
#关闭swap
swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab
systemctl disable firewalld && systemctl stop firewalld

```
##### 开启 IPVS
```bash
#安装 ipset 及 ipvsadm
yum install -y ipset ipvsadm

# 添加需要加载的模块
cat > /etc/sysconfig/modules/ipvs.modules << EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
#Ensure IPVS required kernel modules (Notes: use nf_conntrack instead of nf_conntrack_ipv4 for Linux kernel 4.19 and later)
modprobe -- nf_conntrack_ipv4
modprobe -- nf_conntrack
EOF

# 授权、运行、检查是否加载
chmod +x /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

#### 初始化docker

##### 安装docker
```bash
#安装依赖
yum install -y yum-utils device-mapper-persistent-data lvm2

#重置 docker repo 文件
rm -f /etc/yum.repos.d/docker-ce.repo
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -P /etc/yum.repos.d/

#安装指定版本
yum install docker-ce-${docker_version} docker-ce-cli-${docker_version} containerd.io
```

##### 初始化并启动docker
```bash
#初始化配置
cat > /etc/docker/daemon.json << EOF
{
  "registry-mirrors": ["https://registry.docker-cn.com","https://registry.aliyuncs.com"],
  "insecure-registries":["私有仓库"],
  "max-concurrent-downloads": 10,
  "live-restore": true,
  "log-driver": "local",
  "log-level": "warn",
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-opts": {
    "max-size": "100m",
    "max-file": "5"
    },
  "data-root": "/data/docker"
}
EOF
#创建所需目录
chmod 655 /data/docker/ 
chmod -R 655 /data/docker/containers/

#开机启动
systemctl enable docker
systemctl start docker
```

#### kubernetes安装并启动


##### k8s rpo设置
```bash
cat > /etc/yum.repos.d/k8s.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装kubeadm kubelet kubectl
yum install -y kubeadm-${k8s_version}-0 kubelet-${k8s_version}-0 kubectl-${k8s_version}-0

#docker配置中的 的 cgroup driver 做了修改，这里 kubelet 需要保持一致，需修改如下配置内容
cat > /etc/sysconfig/kubelet << EOF
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
EOF
#解决k8s pull 镜像时的权限问题
sed -i "s/\[Service\]/\[Service\]\\nUser=root/g" /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
#对 kubelet 设置为开机启动即可
systemctl enable kubelet

```
##### 镜像准备
```bash
images=($(kubeadm config images list | grep "k8s.gcr.io"))
for image in "${images[@]}"
do
    image_name=(${image//\// })
    image_addr="registry.aliyuncs.com/google_containers/${image_name[1]}"
    new_image_addr="k8s.gcr.io/${image_name[1]}"
    docker pull ${image_addr}
    docker image tag  ${image_addr} ${new_image_addr}
    docker rmi ${image_addr}
done
```

##### k8s master节点初始化
kubeadm-config.yaml
kubeadm的config配置文件的具体内容可以通过kubeadm config print init-defaults --component-configs KubeProxyConfiguration 来进行查看
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.18.2
networking:
  serviceSubnet: 10.96.0.0/12
imageRepository: registry.aliyuncs.com/google_containers
controlPlaneEndpoint: "${HOST0}:16443"
#使用外部etcd集群
etcd:
    external:
        endpoints:
        - https://${HOST0}:2379
        - https://${HOST1}:2379
        - https://${HOST2}:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
kubernetesVersion: v1.18.2
#开启ipvs
mode: ipvs
```

```bash
kubeadm reset
kubeadm init --config kubeadm-config.yaml --upload-certs --v=5
```
```text
如果看到如下内容表示执行成功

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 10.216.91.128:16443 --token t4s9qv.wjznfvaz2qcdjara \
    --discovery-token-ca-cert-hash sha256:cf9c1e45c9753ccffb6c0dc6452dfec7524ac2e00ab78b4c8e4ee5d49da42cf3 \
    --control-plane --certificate-key 7de17ee8c210558a1ca05cabdb2300f5ccdc3eb069a95f426fb45273b2ddf7bf

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.216.91.128:16443 --token t4s9qv.wjznfvaz2qcdjara \
    --discovery-token-ca-cert-hash sha256:cf9c1e45c9753ccffb6c0dc6452dfec7524ac2e00ab78b4c8e4ee5d49da42cf3
```

##### k8s work节点初始化
```bash
kubeadm reset
rm -rf /var/lib/cni 
rm -rf /etc/cni/net.d
kubeadm join 192.168.0.114:6443 --token 2xjokn.mxcdy3sv2teg5qtr \
    --discovery-token-ca-cert-hash sha256:e0289d4f18dd7f1530bad0863492546fd36d6a43617d7e8388b289f18b41a357
```

#### 其他

##### 删除节点
```bash
kubectl drain ${node_name} --delete-local-data --force --ignore-daemonsets
kubectl delete node  ${node_name}

```

##### 卸载k8s和docker
```bash
yum remove docker-ce
rm -rf /var/lib/docker*
rm -rf /var/lib/cni 
rm -rf /etc/cni/net.d
yum remove kubeadm kubectl kubelet kubernetes-cni kube*
```

##### stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/
这个一般是由于之前卸载k8s不干净导致的，执行以下命令进行清除，然后重新加入集群
```bash
kubeadm reset
rm -rf /var/lib/cni 
rm -rf /etc/cni/net.d
```
